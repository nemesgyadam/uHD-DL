{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "import mne\n",
    "\n",
    "mne.set_log_level('error')\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "from utils.load import Load\n",
    "from config.default import cfg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S2\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S3\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S4\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S5\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n"
     ]
    }
   ],
   "source": [
    "subject_data = {}\n",
    "# Load the data  from the HDF5 file\n",
    "target_dir = 'features'\n",
    "tag = 'reproduced_with_bad'\n",
    "\n",
    "for subject in cfg['subjects']:\n",
    "    file_path = os.path.join(target_dir, tag+'_'+subject + '.h5')\n",
    "\n",
    "    data = {}\n",
    "    with h5py.File(file_path, 'r') as h5file:\n",
    "        for key in h5file.keys():\n",
    "            data[key] = np.array(h5file[key])\n",
    "\n",
    "    subject_data[subject] = data\n",
    "\n",
    "\n",
    "for subject_id in subject_data:\n",
    "    print(subject_id)\n",
    "    print(subject_data[subject_id].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250 samples per subject\n",
    "# 1250 total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(data, batch_size_per_subject):\n",
    "    subject_data = []\n",
    "    subject_labels = []\n",
    "\n",
    "    for subject_id, (subject, data) in enumerate(data.items()):\n",
    "        features = np.concatenate(list(data.values()), axis=0)\n",
    "        labels = []\n",
    "        for i, key in enumerate(data.keys()):\n",
    "            labels.append(np.ones(len(data[key])) * i)\n",
    "        \n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "\n",
    "        features = features.reshape(features.shape[0], -1)\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        indices = np.arange(labels.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # Apply shuffled indices to features and labels\n",
    "        features = features[indices]\n",
    "        labels = labels[indices]\n",
    "\n",
    "        subject_data.append(features)\n",
    "        subject_labels.append(labels)\n",
    "\n",
    "    \n",
    "    mini_batches = []\n",
    "    \n",
    "    for i in range(0, len(subject_data[0]), batch_size_per_subject):\n",
    "\n",
    "        mini_batch = []\n",
    "        mini_features = []\n",
    "        mini_labels = []\n",
    "\n",
    "        for s in range(len(subject_data)):\n",
    "            mini_features.append(subject_data[s][i:i+batch_size_per_subject])\n",
    "            mini_labels.append(subject_labels[s][i:i+batch_size_per_subject])\n",
    "\n",
    "        mini_features = torch.Tensor(np.stack(mini_features, axis=0)).to(device)\n",
    "        mini_labels = torch.Tensor(np.stack(mini_labels, axis=0)).to(device)\n",
    "        mini_featurs = mini_features.permute(1, 0, 2)\n",
    "        mini_labels = mini_labels.permute(0, 1)\n",
    "\n",
    "\n",
    "        mini_batches.append((mini_features, mini_labels))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of minibatches: 25\n",
      "Number of subjects: 5\n"
     ]
    }
   ],
   "source": [
    "mini_batches = create_mini_batches(subject_data, batch_size_per_subject=10)\n",
    "# mini_batches: A list of tuples, with each tuple representing a mini-batch, having a length equal to 'batch_size':\n",
    "#   - Each tuple contains:\n",
    "#       1. batch_data (list): A list of length 'num_subjects', where each element is a PyTorch tensor of shape (batch_size, num_features) representing one subject's data.\n",
    "#       2. batch_labels (tensor): A PyTorch tensor of shape (batch_size * num_subjects,) containing the concatenated labels for all subjects in the mini-batch.\n",
    "\n",
    "print(f'Number of minibatches: {len(mini_batches)}')\n",
    "\n",
    "print(f'Number of subjects: {len(mini_batches[0][0])}')\n",
    "\n",
    "# 25 minibatches\n",
    "# 5 * 10 samples per minibatch (10 samples per subject)\n",
    "# 1280 samples total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mini_batches = mini_batches[:20]\n",
    "test_mini_batches = mini_batches[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_size, subject_hidden_size, shared_hidden_size, num_subjects, output_size):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.subject_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, subject_hidden_size),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(num_subjects)\n",
    "        ])\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            # nn.Linear(subject_hidden_size, shared_hidden_size),\n",
    "            # nn.ReLU(),\n",
    "            #nn.Linear(shared_hidden_size, output_size),\n",
    "            nn.Linear(subject_hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        subject_outputs = []\n",
    "        for x, subject_layer in zip(data, self.subject_layers):\n",
    "\n",
    "\n",
    "            # Pass the input data (x) through the subject-specific layer\n",
    "            hidden_output = subject_layer(x)\n",
    "            \n",
    "            # Pass the output from the subject-specific layer through the shared layer\n",
    "            shared_output = self.shared_layer(hidden_output)\n",
    "            \n",
    "\n",
    "            subject_outputs.append(shared_output)\n",
    "        return torch.stack(subject_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 10\n",
    "input_size = 8216\n",
    "subject_hidden_size = 1\n",
    "shared_hidden_size = 5000\n",
    "num_subjects = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleList: 1-1                        --\n",
      "|    └─Sequential: 2-1                   --\n",
      "|    |    └─Linear: 3-1                  8,217\n",
      "|    |    └─ReLU: 3-2                    --\n",
      "|    └─Sequential: 2-2                   --\n",
      "|    |    └─Linear: 3-3                  8,217\n",
      "|    |    └─ReLU: 3-4                    --\n",
      "|    └─Sequential: 2-3                   --\n",
      "|    |    └─Linear: 3-5                  8,217\n",
      "|    |    └─ReLU: 3-6                    --\n",
      "|    └─Sequential: 2-4                   --\n",
      "|    |    └─Linear: 3-7                  8,217\n",
      "|    |    └─ReLU: 3-8                    --\n",
      "|    └─Sequential: 2-5                   --\n",
      "|    |    └─Linear: 3-9                  8,217\n",
      "|    |    └─ReLU: 3-10                   --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Linear: 2-6                       10\n",
      "=================================================================\n",
      "Total params: 41,095\n",
      "Trainable params: 41,095\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "model = CustomMLP(input_size, subject_hidden_size, shared_hidden_size, num_subjects, output_size)\n",
    "summary(model, input_size=(5, 10, input_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subject_accuracy(model, test_mini_batches):\n",
    "    subject_correct = {}\n",
    "    subject_total_samples = {}\n",
    "    \n",
    "    for batch_data, batch_labels in test_mini_batches:\n",
    "        subject_outputs = model(batch_data)\n",
    "        \n",
    "        for subject_output, subject_label in zip(subject_outputs, batch_labels):\n",
    "            _, predicted = torch.max(subject_output.data, 1)\n",
    "            \n",
    "            for pred, true_label in zip(predicted, subject_label):\n",
    "                true_label = true_label.item()\n",
    "                pred = pred.item()\n",
    "                \n",
    "                if true_label not in subject_total_samples:\n",
    "                    subject_total_samples[true_label] = 0\n",
    "                if true_label not in subject_correct:\n",
    "                    subject_correct[true_label] = 0\n",
    "                \n",
    "                subject_total_samples[true_label] += 1\n",
    "                subject_correct[true_label] += int(pred == true_label)\n",
    "    \n",
    "    subject_accuracy = {}\n",
    "    for subject in subject_total_samples:\n",
    "        subject_accuracy[subject] = subject_correct[subject] / subject_total_samples[subject]\n",
    "        \n",
    "    return subject_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 36.01104915142059, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 2/100, Loss: 35.911216735839844, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 3/100, Loss: 35.82422959804535, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 4/100, Loss: 35.74298298358917, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 5/100, Loss: 35.66886007785797, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 6/100, Loss: 35.60084044933319, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 7/100, Loss: 35.53451216220856, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 8/100, Loss: 35.466535806655884, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 9/100, Loss: 35.399110555648804, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 10/100, Loss: 35.34178423881531, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 11/100, Loss: 35.273690819740295, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 12/100, Loss: 35.212339997291565, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 13/100, Loss: 35.14848506450653, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 14/100, Loss: 35.08776581287384, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 15/100, Loss: 35.02934420108795, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 16/100, Loss: 34.96939516067505, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 17/100, Loss: 34.90658903121948, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 18/100, Loss: 34.84740447998047, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 19/100, Loss: 34.79217004776001, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 20/100, Loss: 34.729419589042664, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 21/100, Loss: 34.68357586860657, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 22/100, Loss: 34.614102602005005, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 23/100, Loss: 34.560983538627625, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 24/100, Loss: 34.50104796886444, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 25/100, Loss: 34.449676156044006, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 26/100, Loss: 34.38766622543335, Accuracy: 0.205, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 1.0, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 27/100, Loss: 34.32387173175812, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 28/100, Loss: 34.273494362831116, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 29/100, Loss: 34.20682966709137, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 30/100, Loss: 34.151602149009705, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 31/100, Loss: 34.09281539916992, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 32/100, Loss: 34.037309408187866, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 33/100, Loss: 33.981669902801514, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 34/100, Loss: 33.92522120475769, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 35/100, Loss: 33.86647713184357, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 36/100, Loss: 33.81099534034729, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 37/100, Loss: 33.755414843559265, Accuracy: 0.204, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 38/100, Loss: 33.7015438079834, Accuracy: 0.204, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 39/100, Loss: 33.64489161968231, Accuracy: 0.204, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 40/100, Loss: 33.5880651473999, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 41/100, Loss: 33.530123233795166, Accuracy: 0.205, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 42/100, Loss: 33.47513997554779, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 43/100, Loss: 33.42064189910889, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 44/100, Loss: 33.36501085758209, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 45/100, Loss: 33.307648062705994, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 46/100, Loss: 33.254088044166565, Accuracy: 0.206, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 47/100, Loss: 33.20253610610962, Accuracy: 0.207, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 48/100, Loss: 33.14418709278107, Accuracy: 0.208, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 49/100, Loss: 33.089563488960266, Accuracy: 0.208, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 50/100, Loss: 33.035218954086304, Accuracy: 0.209, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 51/100, Loss: 32.982038497924805, Accuracy: 0.208, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 52/100, Loss: 32.9306036233902, Accuracy: 0.208, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 53/100, Loss: 32.877541303634644, Accuracy: 0.209, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 54/100, Loss: 32.82302463054657, Accuracy: 0.213, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 55/100, Loss: 32.77276396751404, Accuracy: 0.216, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 56/100, Loss: 32.722835183143616, Accuracy: 0.216, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 57/100, Loss: 32.66968035697937, Accuracy: 0.217, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 58/100, Loss: 32.616156339645386, Accuracy: 0.216, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 59/100, Loss: 32.56747567653656, Accuracy: 0.217, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 60/100, Loss: 32.5143541097641, Accuracy: 0.217, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 61/100, Loss: 32.46334099769592, Accuracy: 0.217, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 62/100, Loss: 32.41331744194031, Accuracy: 0.219, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 63/100, Loss: 32.36420714855194, Accuracy: 0.22, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 64/100, Loss: 32.316338419914246, Accuracy: 0.221, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 65/100, Loss: 32.264914870262146, Accuracy: 0.221, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 66/100, Loss: 32.21596848964691, Accuracy: 0.221, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 67/100, Loss: 32.167572021484375, Accuracy: 0.224, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 68/100, Loss: 32.11617910861969, Accuracy: 0.228, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 69/100, Loss: 32.06999623775482, Accuracy: 0.233, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.0, 2.0: 0.9777777777777777, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 70/100, Loss: 32.02127277851105, Accuracy: 0.236, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.016666666666666666, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 71/100, Loss: 31.975406169891357, Accuracy: 0.241, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.016666666666666666, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 72/100, Loss: 31.925478219985962, Accuracy: 0.247, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.016666666666666666, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 73/100, Loss: 31.87746036052704, Accuracy: 0.247, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 74/100, Loss: 31.831368684768677, Accuracy: 0.246, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 75/100, Loss: 31.7825767993927, Accuracy: 0.247, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 76/100, Loss: 31.738500356674194, Accuracy: 0.25, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 77/100, Loss: 31.690094470977783, Accuracy: 0.25, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 78/100, Loss: 31.644670128822327, Accuracy: 0.25, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 79/100, Loss: 31.5965633392334, Accuracy: 0.253, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 80/100, Loss: 31.55304181575775, Accuracy: 0.253, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 81/100, Loss: 31.504595279693604, Accuracy: 0.254, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 82/100, Loss: 31.461572289466858, Accuracy: 0.256, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 83/100, Loss: 31.414013743400574, Accuracy: 0.257, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 84/100, Loss: 31.372284412384033, Accuracy: 0.26, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 85/100, Loss: 31.326316595077515, Accuracy: 0.262, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 86/100, Loss: 31.282636761665344, Accuracy: 0.261, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 87/100, Loss: 31.239364981651306, Accuracy: 0.263, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 88/100, Loss: 31.19623839855194, Accuracy: 0.266, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 89/100, Loss: 31.155361533164978, Accuracy: 0.269, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 90/100, Loss: 31.10844326019287, Accuracy: 0.27, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 91/100, Loss: 31.066296458244324, Accuracy: 0.271, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 92/100, Loss: 31.026041388511658, Accuracy: 0.274, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 93/100, Loss: 30.982038140296936, Accuracy: 0.274, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 94/100, Loss: 30.942575097084045, Accuracy: 0.278, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.03333333333333333, 2.0: 0.9555555555555556, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 95/100, Loss: 30.901928901672363, Accuracy: 0.278, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.05, 2.0: 0.9333333333333333, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 96/100, Loss: 30.85922360420227, Accuracy: 0.281, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.05, 2.0: 0.9111111111111111, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 97/100, Loss: 30.819285035133362, Accuracy: 0.283, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.06666666666666667, 2.0: 0.9111111111111111, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 98/100, Loss: 30.779416918754578, Accuracy: 0.282, Test Accuracy: 0.18\n",
      "{4.0: 0.0, 1.0: 0.06666666666666667, 2.0: 0.9111111111111111, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 99/100, Loss: 30.739556670188904, Accuracy: 0.283, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.05, 2.0: 0.9111111111111111, 3.0: 0.0, 0.0: 0.0}\n",
      "Epoch 100/100, Loss: 30.699798345565796, Accuracy: 0.283, Test Accuracy: 0.176\n",
      "{4.0: 0.0, 1.0: 0.05, 2.0: 0.9111111111111111, 3.0: 0.0, 0.0: 0.0}\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    epoch_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    for i, (batch_data, batch_labels) in enumerate(train_mini_batches):\n",
    "\n",
    "        #print(f'batch_data_shape: {batch_data.shape}')\n",
    "        #print(f'batch_labels_shape: {batch_labels.shape}')\n",
    " \n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagate the data through each subject's first layer and then through the shared second layer\n",
    "        subject_outputs = model(batch_data)\n",
    "        #print(f'subject_outputs_shape: {subject_outputs.shape}')\n",
    "\n",
    "\n",
    "        # Create an empty list to store the losses for each subject\n",
    "        losses = []\n",
    "        # Iterate over the subject outputs and ground-truth labels\n",
    "        for subject_output, subject_labels in zip(subject_outputs, batch_labels):\n",
    "            loss = criterion(subject_output, subject_labels.long())\n",
    "            losses.append(loss)\n",
    "\n",
    "        # Calculate the average loss across subjects\n",
    "        loss = torch.mean(torch.stack(losses))\n",
    "\n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy\n",
    "        \n",
    "     \n",
    "        for subject_output, subject_label in zip(subject_outputs, batch_labels):\n",
    "            _, predicted = torch.max(subject_output.data, 1)\n",
    "            total_samples += subject_label.size(0)\n",
    "            correct += (predicted == subject_label).sum().item()\n",
    "    epoch_accuracy += correct / total_samples\n",
    "\n",
    "\n",
    "    test_epoch_accuracy = 0.0\n",
    "    test_total_samples = 0\n",
    "    test_correct = 0\n",
    "    for batch_data, batch_labels in test_mini_batches:\n",
    "            subject_outputs = model(batch_data)\n",
    "            for subject_output, subject_label in zip(subject_outputs, batch_labels):\n",
    "                _, predicted = torch.max(subject_output.data, 1)\n",
    "                test_total_samples += subject_label.size(0)\n",
    "                test_correct += (predicted == subject_label).sum().item()\n",
    "    test_epoch_accuracy += test_correct / test_total_samples\n",
    "\n",
    "      \n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}, Test Accuracy: {test_epoch_accuracy}\")\n",
    "    print(calculate_subject_accuracy(model, test_mini_batches))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cefe38f745df9e33a66570f2e5a410ba71c4ae3bf929b6ad1b474ac5f904d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
