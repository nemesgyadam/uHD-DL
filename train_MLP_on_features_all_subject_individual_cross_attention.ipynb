{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "import mne\n",
    "import math \n",
    "mne.set_log_level('error')\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "from utils.load import Load\n",
    "from config.default import cfg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S2\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S3\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S4\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S5\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n"
     ]
    }
   ],
   "source": [
    "subject_data = {}\n",
    "# Load the data  from the HDF5 file\n",
    "target_dir = 'features'\n",
    "tag = 'reproduced_with_bad'\n",
    "\n",
    "for subject in cfg['subjects']:\n",
    "    file_path = os.path.join(target_dir, tag+'_'+subject + '.h5')\n",
    "\n",
    "    data = {}\n",
    "    with h5py.File(file_path, 'r') as h5file:\n",
    "        for key in h5file.keys():\n",
    "            data[key] = np.array(h5file[key])\n",
    "\n",
    "    subject_data[subject] = data\n",
    "\n",
    "\n",
    "for subject_id in subject_data:\n",
    "    print(subject_id)\n",
    "    print(subject_data[subject_id].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250 samples per subject\n",
    "# 1250 total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subject_data, train_percent=0.8, seed=42, device=None, is_train=True):\n",
    "        self.device = device\n",
    "        self.is_train = is_train\n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self.preprocess_data(subject_data, train_percent, seed)\n",
    "\n",
    "\n",
    "    def preprocess_data(self, subject_data, train_percent, seed):\n",
    "        np.random.seed(seed)\n",
    "        global_train_features = []\n",
    "        global_train_labels = []\n",
    "        global_test_features = []\n",
    "        global_test_labels = []\n",
    "\n",
    "        for s, subject_id in enumerate(subject_data):\n",
    "            data = subject_data[subject_id]\n",
    "\n",
    "            for i, finger in enumerate(data):\n",
    "                finger_data = data[finger]\n",
    "\n",
    "                # Reshape\n",
    "                finger_data = finger_data.reshape(finger_data.shape[0], -1)\n",
    "\n",
    "                # Normalize (uncomment if needed)\n",
    "                # finger_data = StandardScaler().fit_transform(finger_data)\n",
    "\n",
    "                np.random.shuffle(finger_data)\n",
    "\n",
    "                # To GPU\n",
    "                finger_data = torch.tensor(finger_data).to(torch.float32).to(self.device)\n",
    "                features = [(finger_data[d], ids[d]) for d in range(len(finger_data))]\n",
    "\n",
    "                ids = torch.tensor(np.ones((len(finger_data))) * s).to(torch.int64).to(self.device)\n",
    "                labels = torch.tensor(np.ones((len(finger_data))) * i).to(self.device)\n",
    "\n",
    "                # Split\n",
    "                train_features = features[:int(len(finger_data) * train_percent)]\n",
    "                train_labels = labels[:int(len(finger_data) * train_percent)]\n",
    "                test_features = features[int(len(finger_data) * train_percent):]\n",
    "                test_labels = labels[int(len(finger_data) * train_percent):]\n",
    "\n",
    "                global_train_features.extend(train_features)\n",
    "                global_train_labels.extend(train_labels)\n",
    "                global_test_features.extend(test_features)\n",
    "                global_test_labels.extend(test_labels)\n",
    "\n",
    "        return global_train_features, global_train_labels, global_test_features, global_test_labels\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_y) if self.is_train else len(self.test_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.get_train_item(idx)\n",
    "        else:\n",
    "            return self.get_test_item(idx)\n",
    "\n",
    "    def get_train_item(self, idx):\n",
    "        features = self.train_X[idx][0]\n",
    "        subject_id = self.train_X[idx][1]\n",
    "        label = self.train_y[idx]\n",
    "\n",
    "        return (features, subject_id), label\n",
    "\n",
    "    def get_test_item(self, idx):\n",
    "        features = self.test_X[idx][0]\n",
    "        subject_id = self.test_X[idx][1]\n",
    "        label = self.test_y[idx]\n",
    "\n",
    "        return (features, subject_id), label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(subject_data, device=device, is_train=True)\n",
    "test_dataset = CustomDataset(subject_data, device=device, is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = ([EEG], [subject_id])\n",
    "# label = finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8216])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for i, (feature, label) in enumerate(train_dataloader):\n",
    "    print(feature[0].shape)\n",
    "    print(feature[1].shape)\n",
    "    print(label.shape)\n",
    "    print('---------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim, user_dim, hidden_dim, feautre_hidden_dim,  num_classes):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.user_dim = user_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_hidden_dim = feautre_hidden_dim\n",
    "        self.num_queries = 3\n",
    "\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'feature extractor' : nn.Linear(self.feature_dim,  self.feature_hidden_dim),\n",
    "            'batch norm': nn.BatchNorm1d(self.feature_hidden_dim),\n",
    "            'query layer' : nn.Linear( self.feature_hidden_dim, self.feature_hidden_dim * self.hidden_dim ),  # Query transformation\n",
    "            'key layer' : nn.Linear(self.user_dim, self.hidden_dim *self.user_dim ),  # Key transformation\n",
    "            'value layer': nn.Linear(self.user_dim, self.hidden_dim *self.user_dim ),  # Value transformation\n",
    "            'dropout': nn.Dropout(0.2),\n",
    "            'classifier' : nn.Linear(self.feature_hidden_dim * self.hidden_dim, num_classes)\n",
    "        })\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, features, user_indices):\n",
    "        # Transform features to hidden_dim\n",
    "        features = self.layers['feature extractor'](features)  \n",
    "        features = self.layers['batch norm'](features)\n",
    "       \n",
    "        # Convert user_indices to one_hot vectors\n",
    "        user_one_hot = torch.zeros(user_indices.size(0), self.user_dim, device=user_indices.device)\n",
    "        user_one_hot.scatter_(1, user_indices.unsqueeze(1), 1)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "        # Transform user_one_hot to query\n",
    "        query = self.layers['query layer'](features)  \n",
    "        query = query.view(-1, self.feature_hidden_dim, self.hidden_dim )  \n",
    "\n",
    "\n",
    "        # Transform features to keys and values\n",
    "        keys = self.layers['key layer'](user_one_hot)  \n",
    "        keys = keys.view(-1,  self.hidden_dim, self.user_dim)  \n",
    "        values = self.layers['value layer'](user_one_hot) \n",
    "        values = values.view(-1,  self.user_dim, self.hidden_dim) \n",
    "        # print(f'query: {query.shape}')\n",
    "        # print(f'keys: {keys.shape}')\n",
    "        # print(f'values: {values.shape}')\n",
    "\n",
    "       \n",
    "        # Calculate attention scores\n",
    "        attention_scores = torch.bmm(query, keys) \n",
    "\n",
    "        # Normalize the attention scores\n",
    "        attention_scores = attention_scores / math.sqrt(self.hidden_dim)\n",
    "\n",
    "        attention_probs = self.softmax(attention_scores) \n",
    "     \n",
    "        # Calculate the attended features\n",
    "        attended_features = torch.bmm(attention_probs, values) \n",
    "        attended_features = attended_features.view(-1, self.feature_hidden_dim * self.hidden_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        attended_features = self.layers['dropout'](attended_features)\n",
    "        \n",
    "        # Classify the attended features\n",
    "        output = self.layers['classifier'](attended_features)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 8216\n",
    "hidden_dim = 2\n",
    "feautre_hidden_dim = 8\n",
    "\n",
    "num_subjects = 5\n",
    "output_size = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "epochs = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleDict: 1-1                        --\n",
      "|    └─Linear: 2-1                       65,736\n",
      "|    └─BatchNorm1d: 2-2                  16\n",
      "|    └─Linear: 2-3                       144\n",
      "|    └─Linear: 2-4                       60\n",
      "|    └─Linear: 2-5                       60\n",
      "|    └─Dropout: 2-6                      --\n",
      "|    └─Linear: 2-7                       85\n",
      "├─Softmax: 1-2                           --\n",
      "=================================================================\n",
      "Total params: 66,101\n",
      "Trainable params: 66,101\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create model and pass data\n",
    "model = CrossAttention(input_size, num_subjects, hidden_dim, feautre_hidden_dim, output_size)\n",
    "model.to(device)\n",
    "summary(model, input_size=(5, 10, input_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features,  labels in dataloader:\n",
    "\n",
    "            outputs = model(features = features[0], user_indices = features[1])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy * 100\n",
    "    #print(f\"Accuracy: {accuracy * 100:.2f}% ({correct_predictions}/{total_predictions})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 56.08420732617378, Train accuracy: 68.80%, Test accuracy: 58.80%\n",
      "Epoch 20/200, Loss: 20.382360510528088, Train accuracy: 89.10%, Test accuracy: 74.40%\n",
      "Epoch 30/200, Loss: 11.554575692862272, Train accuracy: 96.50%, Test accuracy: 79.20%\n",
      "Epoch 40/200, Loss: 8.316255658399314, Train accuracy: 95.60%, Test accuracy: 81.60%\n",
      "Epoch 50/200, Loss: 6.187756382394582, Train accuracy: 98.10%, Test accuracy: 82.80%\n",
      "Epoch 60/200, Loss: 4.921193789690733, Train accuracy: 91.90%, Test accuracy: 78.40%\n",
      "Epoch 70/200, Loss: 3.416063972050324, Train accuracy: 98.50%, Test accuracy: 84.40%\n",
      "Epoch 80/200, Loss: 1.2954519583145157, Train accuracy: 97.50%, Test accuracy: 83.60%\n",
      "Epoch 90/200, Loss: 3.3669168053893372, Train accuracy: 99.00%, Test accuracy: 82.80%\n",
      "Epoch 100/200, Loss: 1.2739150493289344, Train accuracy: 99.80%, Test accuracy: 84.80%\n",
      "Epoch 110/200, Loss: 2.986518845485989, Train accuracy: 99.80%, Test accuracy: 84.00%\n",
      "Epoch 120/200, Loss: 2.827765849826392, Train accuracy: 99.50%, Test accuracy: 85.20%\n",
      "Epoch 130/200, Loss: 3.809209245024249, Train accuracy: 99.70%, Test accuracy: 83.60%\n",
      "Epoch 140/200, Loss: 2.5551920438883826, Train accuracy: 99.80%, Test accuracy: 83.60%\n",
      "Epoch 150/200, Loss: 0.6329999271038105, Train accuracy: 99.80%, Test accuracy: 83.20%\n",
      "Epoch 160/200, Loss: 1.3550307867699303, Train accuracy: 99.40%, Test accuracy: 82.00%\n",
      "Epoch 170/200, Loss: 0.4763402498792857, Train accuracy: 99.90%, Test accuracy: 82.80%\n",
      "Epoch 180/200, Loss: 0.48243480948440265, Train accuracy: 99.80%, Test accuracy: 83.20%\n",
      "Epoch 190/200, Loss: 2.0325031850952655, Train accuracy: 99.30%, Test accuracy: 81.20%\n",
      "Epoch 200/200, Loss: 0.6303063907544129, Train accuracy: 100.00%, Test accuracy: 82.40%\n",
      "##################################################\n",
      "Final_loss: 0.6303063907544129\n",
      "Final train accuracy: 100.00%\n",
      "Final test accuracy: 82.40%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_features, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features = batch_features[0], user_indices = batch_features[1])\n",
    "        loss = criterion(outputs, batch_labels.long())\n",
    "          \n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "   \n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        train_accuracy = accuracy(train_dataloader)\n",
    "        test_accuracy = accuracy(test_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}, Train accuracy: {train_accuracy:.2f}%, Test accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(f'Final_loss: {epoch_loss}')\n",
    "print(f'Final train accuracy: {accuracy(train_dataloader):.2f}%')\n",
    "print(f'Final test accuracy: {accuracy(test_dataloader):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cefe38f745df9e33a66570f2e5a410ba71c4ae3bf929b6ad1b474ac5f904d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
