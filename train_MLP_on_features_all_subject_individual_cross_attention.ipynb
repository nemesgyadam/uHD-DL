{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "import mne\n",
    "import math \n",
    "mne.set_log_level('error')\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "from utils.load import Load\n",
    "from config.default import cfg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S2\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S3\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S4\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S5\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n"
     ]
    }
   ],
   "source": [
    "subject_data = {}\n",
    "# Load the data  from the HDF5 file\n",
    "target_dir = 'features'\n",
    "tag = 'reproduced_with_bad'\n",
    "\n",
    "for subject in cfg['subjects']:\n",
    "    file_path = os.path.join(target_dir, tag+'_'+subject + '.h5')\n",
    "\n",
    "    data = {}\n",
    "    with h5py.File(file_path, 'r') as h5file:\n",
    "        for key in h5file.keys():\n",
    "            data[key] = np.array(h5file[key])\n",
    "\n",
    "    subject_data[subject] = data\n",
    "\n",
    "\n",
    "for subject_id in subject_data:\n",
    "    print(subject_id)\n",
    "    print(subject_data[subject_id].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250 samples per subject\n",
    "# 1250 total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subject_data, train_percent=0.8, seed=42, device=None, is_train=True):\n",
    "        self.device = device\n",
    "        self.is_train = is_train\n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self.preprocess_data(subject_data, train_percent, seed)\n",
    "\n",
    "\n",
    "    def preprocess_data(self, subject_data, train_percent, seed):\n",
    "        np.random.seed(seed)\n",
    "        global_train_features = []\n",
    "        global_train_labels = []\n",
    "        global_test_features = []\n",
    "        global_test_labels = []\n",
    "\n",
    "        for s, subject_id in enumerate(subject_data):\n",
    "            data = subject_data[subject_id]\n",
    "\n",
    "            for i, finger in enumerate(data):\n",
    "                finger_data = data[finger]\n",
    "\n",
    "                # Reshape\n",
    "                finger_data = finger_data.reshape(finger_data.shape[0], -1)\n",
    "\n",
    "                # Normalize (uncomment if needed)\n",
    "                # finger_data = StandardScaler().fit_transform(finger_data)\n",
    "\n",
    "\n",
    "                ids = torch.tensor(np.ones((len(finger_data))) * s).to(torch.int64).to(self.device)\n",
    "                labels = torch.tensor(np.ones((len(finger_data))) * i).to(self.device)\n",
    "\n",
    "                # To GPU\n",
    "                finger_data = torch.tensor(finger_data).to(torch.float32).to(self.device)\n",
    "                features = [(finger_data[d], ids[d]) for d in range(len(finger_data))]\n",
    "\n",
    "                # Split\n",
    "                train_features = features[:int(len(finger_data) * train_percent)]\n",
    "                train_labels = labels[:int(len(finger_data) * train_percent)]\n",
    "                test_features = features[int(len(finger_data) * train_percent):]\n",
    "                test_labels = labels[int(len(finger_data) * train_percent):]\n",
    "\n",
    "\n",
    "\n",
    "                global_train_features.extend(train_features)\n",
    "                global_train_labels.extend(train_labels)\n",
    "                global_test_features.extend(test_features)\n",
    "                global_test_labels.extend(test_labels)\n",
    "\n",
    "        # Shuffle\n",
    "        global_train_features, global_train_labels = shuffle(global_train_features, global_train_labels)\n",
    "        global_test_features, global_test_labels = shuffle(global_test_features, global_test_labels)\n",
    "\n",
    "        return global_train_features, global_train_labels, global_test_features, global_test_labels\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_y) if self.is_train else len(self.test_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.get_train_item(idx)\n",
    "        else:\n",
    "            return self.get_test_item(idx)\n",
    "\n",
    "    def get_train_item(self, idx):\n",
    "        features = self.train_X[idx][0]\n",
    "        subject_id = self.train_X[idx][1]\n",
    "        label = self.train_y[idx]\n",
    "\n",
    "        return (features, subject_id), label\n",
    "\n",
    "    def get_test_item(self, idx):\n",
    "        features = self.test_X[idx][0]\n",
    "        subject_id = self.test_X[idx][1]\n",
    "        label = self.test_y[idx]\n",
    "\n",
    "        return (features, subject_id), label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(subject_data, device=device, is_train=True)\n",
    "test_dataset = CustomDataset(subject_data, device=device, is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = ([EEG], [subject_id])\n",
    "# label = finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8216])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for i, (feature, label) in enumerate(train_dataloader):\n",
    "    print(feature[0].shape)\n",
    "    print(feature[1].shape)\n",
    "    print(label.shape)\n",
    "    print('---------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 8216\n",
    "hidden_dim = 2\n",
    "feautre_hidden_dim = 8\n",
    "\n",
    "num_subjects = 5\n",
    "output_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, feature_dim, user_dim, hidden_dim, feautre_hidden_dim,  num_classes):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.user_dim = user_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_hidden_dim = feautre_hidden_dim\n",
    "        self.num_queries = 3\n",
    "\n",
    "\n",
    "        #user_dim -> condition_dim\n",
    "        # feature_dim -> EEG_dim\n",
    "        self.embed_dim = 4\n",
    "        self.k_dim = 4\n",
    "        self.v_dim = 4\n",
    "\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'feature extractor' : nn.Linear(self.feature_dim,  self.feature_hidden_dim),\n",
    "            'batch norm': nn.BatchNorm1d(self.feature_hidden_dim),\n",
    "            'query layer' : nn.Linear( self.feature_hidden_dim, self.embed_dim),      # Query transformation\n",
    "            'key layer' : nn.Linear(self.user_dim, self.k_dim  ),   # Key transformation\n",
    "            'value layer': nn.Linear(self.user_dim, self.v_dim  ),  # Value transformation\n",
    "            'dropout': nn.Dropout(0.2),\n",
    "            'classifier' : nn.Linear(self.feature_hidden_dim * self.embed_dim, num_classes)\n",
    "        })\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, features, user_indices):\n",
    "        \n",
    "        # Convert user_indices to one_hot vectors\n",
    "        user_one_hot = torch.zeros(user_indices.size(0), self.user_dim, device=user_indices.device)\n",
    "        user_one_hot.scatter_(1, user_indices.unsqueeze(1), 1)\n",
    "\n",
    "        # Transform features to hidden_dim\n",
    "        features = self.layers['feature extractor'](features)  \n",
    "        features = self.layers['batch norm'](features)\n",
    "       \n",
    "\n",
    "        # Query Matrix\n",
    "        d_q = self.layers['query layer'](features)\n",
    "        d_q = torch.unsqueeze(input=d_q, dim=1)\n",
    "        features = torch.unsqueeze(input=features, dim=2)  \n",
    "        query = torch.bmm(features, d_q)\n",
    "\n",
    "\n",
    "        # Key Matrix\n",
    "        d_k = self.layers['key layer'](user_one_hot)  \n",
    "        d_k = torch.unsqueeze(input=d_k, dim=1)\n",
    "        user_one_hot = torch.unsqueeze(input=user_one_hot, dim=2)\n",
    "        key = torch.bmm(user_one_hot, d_k)\n",
    "        user_one_hot = torch.square(input=user_one_hot)\n",
    "        key = torch.transpose(key, 1, 2)\n",
    "\n",
    "       \n",
    "        # Attention\n",
    "        attention_scores = torch.bmm(query, key) \n",
    "        attention_scores = attention_scores / math.sqrt(self.embed_dim)\n",
    "        attention_probs = self.softmax(attention_scores) \n",
    "       \n",
    "        # Value Matrix\n",
    "        user_one_hot = torch.squeeze(user_one_hot)\n",
    "        d_v = self.layers['value layer'](user_one_hot) \n",
    "        user_one_hot = torch.unsqueeze(input=user_one_hot, dim=2)\n",
    "        d_v = torch.unsqueeze(input=d_v, dim=1)\n",
    "        values = torch.bmm(user_one_hot, d_v)\n",
    "    \n",
    "       \n",
    "        # Calculate the attended features\n",
    "        attended_features = torch.bmm(attention_probs, values) \n",
    "        batch_size = attended_features.size(0)\n",
    "        attended_features = attended_features.view(batch_size, -1)\n",
    "        \n",
    "        # Dropout\n",
    "        attended_features = self.layers['dropout'](attended_features)\n",
    "        \n",
    "        # Classify the attended features\n",
    "        output = self.layers['classifier'](attended_features)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleDict: 1-1                        --\n",
      "|    └─Linear: 2-1                       65,736\n",
      "|    └─BatchNorm1d: 2-2                  16\n",
      "|    └─Linear: 2-3                       36\n",
      "|    └─Linear: 2-4                       24\n",
      "|    └─Linear: 2-5                       24\n",
      "|    └─Dropout: 2-6                      --\n",
      "|    └─Linear: 2-7                       165\n",
      "├─Softmax: 1-2                           --\n",
      "=================================================================\n",
      "Total params: 66,001\n",
      "Trainable params: 66,001\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create model and pass data\n",
    "model = CrossAttention(input_size, num_subjects, hidden_dim, feautre_hidden_dim, output_size)\n",
    "model.to(device)\n",
    "summary(model, input_size=(5, 10, input_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (feature, label) in enumerate(train_dataloader):\n",
    "    model(feature[0], feature[1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features,  labels in dataloader:\n",
    "\n",
    "            outputs = model(features = features[0], user_indices = features[1])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy * 100\n",
    "    #print(f\"Accuracy: {accuracy * 100:.2f}% ({correct_predictions}/{total_predictions})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 67.65774589776993, Train accuracy: 55.60%, Test accuracy: 21.60%\n",
      "Epoch 20/200, Loss: 65.16666907072067, Train accuracy: 57.10%, Test accuracy: 21.60%\n",
      "Epoch 30/200, Loss: 70.23433631658554, Train accuracy: 57.50%, Test accuracy: 18.80%\n",
      "Epoch 40/200, Loss: 61.79432076215744, Train accuracy: 60.40%, Test accuracy: 20.00%\n",
      "Epoch 50/200, Loss: 61.38509237766266, Train accuracy: 60.80%, Test accuracy: 23.60%\n",
      "Epoch 60/200, Loss: 62.58268052339554, Train accuracy: 58.00%, Test accuracy: 22.80%\n",
      "Epoch 70/200, Loss: 59.481901705265045, Train accuracy: 59.20%, Test accuracy: 23.20%\n",
      "Epoch 80/200, Loss: 65.20154601335526, Train accuracy: 56.30%, Test accuracy: 24.40%\n",
      "Epoch 90/200, Loss: 63.99209460616112, Train accuracy: 55.80%, Test accuracy: 25.20%\n",
      "Epoch 100/200, Loss: 64.48908042907715, Train accuracy: 57.60%, Test accuracy: 24.80%\n",
      "Epoch 110/200, Loss: 63.35892075300217, Train accuracy: 58.60%, Test accuracy: 26.80%\n",
      "Epoch 120/200, Loss: 68.09774720668793, Train accuracy: 55.00%, Test accuracy: 24.00%\n",
      "Epoch 130/200, Loss: 69.56124663352966, Train accuracy: 52.60%, Test accuracy: 22.80%\n",
      "Epoch 140/200, Loss: 67.0486718416214, Train accuracy: 56.40%, Test accuracy: 24.40%\n",
      "Epoch 150/200, Loss: 64.26947170495987, Train accuracy: 57.80%, Test accuracy: 26.00%\n",
      "Epoch 160/200, Loss: 71.42393356561661, Train accuracy: 54.10%, Test accuracy: 20.80%\n",
      "Epoch 170/200, Loss: 69.28562968969345, Train accuracy: 54.90%, Test accuracy: 25.60%\n",
      "Epoch 180/200, Loss: 65.55315321683884, Train accuracy: 57.10%, Test accuracy: 26.80%\n",
      "Epoch 190/200, Loss: 66.22332829236984, Train accuracy: 56.60%, Test accuracy: 22.40%\n",
      "Epoch 200/200, Loss: 66.8520240187645, Train accuracy: 54.70%, Test accuracy: 23.20%\n",
      "##################################################\n",
      "Final_loss: 66.8520240187645\n",
      "Final train accuracy: 54.70%\n",
      "Final test accuracy: 23.20%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_features, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features = batch_features[0], user_indices = batch_features[1])\n",
    "\n",
    "        loss = criterion(outputs, batch_labels.long())\n",
    "          \n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "   \n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        train_accuracy = accuracy(train_dataloader)\n",
    "        test_accuracy = accuracy(test_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}, Train accuracy: {train_accuracy:.2f}%, Test accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(f'Final_loss: {epoch_loss}')\n",
    "print(f'Final train accuracy: {accuracy(train_dataloader):.2f}%')\n",
    "print(f'Final test accuracy: {accuracy(test_dataloader):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cefe38f745df9e33a66570f2e5a410ba71c4ae3bf929b6ad1b474ac5f904d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
