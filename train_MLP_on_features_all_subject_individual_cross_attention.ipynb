{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "import mne\n",
    "import math \n",
    "mne.set_log_level('error')\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "from utils.load import Load\n",
    "from config.default import cfg\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S2\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S3\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S4\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n",
      "S5\n",
      "dict_keys(['index', 'little', 'middle', 'ring', 'thumb'])\n"
     ]
    }
   ],
   "source": [
    "subject_data = {}\n",
    "# Load the data  from the HDF5 file\n",
    "target_dir = 'features'\n",
    "tag = 'reproduced_with_bad'\n",
    "\n",
    "for subject in cfg['subjects']:\n",
    "    file_path = os.path.join(target_dir, tag+'_'+subject + '.h5')\n",
    "\n",
    "    data = {}\n",
    "    with h5py.File(file_path, 'r') as h5file:\n",
    "        for key in h5file.keys():\n",
    "            data[key] = np.array(h5file[key])\n",
    "\n",
    "    subject_data[subject] = data\n",
    "\n",
    "\n",
    "for subject_id in subject_data:\n",
    "    print(subject_id)\n",
    "    print(subject_data[subject_id].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250 samples per subject\n",
    "# 1250 total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subject_data, train_percent=0.8, seed=42, device=None, is_train=True):\n",
    "        self.device = device\n",
    "        self.is_train = is_train\n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self.preprocess_data(subject_data, train_percent, seed)\n",
    "        self.dim = self.train_X[0][0].shape\n",
    "\n",
    "    def get_dim():\n",
    "        return self.dim\n",
    "\n",
    "    def preprocess_data(self, subject_data, train_percent, seed):\n",
    "        np.random.seed(seed)\n",
    "        global_train_features = []\n",
    "        global_train_labels = []\n",
    "        global_test_features = []\n",
    "        global_test_labels = []\n",
    "\n",
    "        for s, subject_id in enumerate(subject_data):\n",
    "            data = subject_data[subject_id]\n",
    "\n",
    "            for i, finger in enumerate(data):\n",
    "                finger_data = data[finger]\n",
    "\n",
    "               \n",
    "\n",
    "                # Normalize (uncomment if needed)\n",
    "                # finger_data = StandardScaler().fit_transform(finger_data)\n",
    "\n",
    "\n",
    "                ids = torch.tensor(np.ones((len(finger_data))) * s).to(torch.int64).to(self.device)\n",
    "                labels = torch.tensor(np.ones((len(finger_data))) * i).to(self.device)\n",
    "\n",
    "                # To GPU\n",
    "                finger_data = torch.tensor(finger_data).to(torch.float32).to(self.device)\n",
    "                features = [(finger_data[d], ids[d]) for d in range(len(finger_data))]\n",
    "\n",
    "                # Split\n",
    "                train_features = features[:int(len(finger_data) * train_percent)]\n",
    "                train_labels = labels[:int(len(finger_data) * train_percent)]\n",
    "                test_features = features[int(len(finger_data) * train_percent):]\n",
    "                test_labels = labels[int(len(finger_data) * train_percent):]\n",
    "\n",
    "\n",
    "\n",
    "                global_train_features.extend(train_features)\n",
    "                global_train_labels.extend(train_labels)\n",
    "                global_test_features.extend(test_features)\n",
    "                global_test_labels.extend(test_labels)\n",
    "\n",
    "        # Shuffle\n",
    "        global_train_features, global_train_labels = shuffle(global_train_features, global_train_labels)\n",
    "        global_test_features, global_test_labels = shuffle(global_test_features, global_test_labels)\n",
    "\n",
    "        return global_train_features, global_train_labels, global_test_features, global_test_labels\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_y) if self.is_train else len(self.test_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.get_train_item(idx)\n",
    "        else:\n",
    "            return self.get_test_item(idx)\n",
    "\n",
    "    def get_train_item(self, idx):\n",
    "        features = self.train_X[idx][0]\n",
    "        subject_id = self.train_X[idx][1]\n",
    "        label = self.train_y[idx]\n",
    "\n",
    "        return (features, subject_id), label\n",
    "\n",
    "    def get_test_item(self, idx):\n",
    "        features = self.test_X[idx][0]\n",
    "        subject_id = self.test_X[idx][1]\n",
    "        label = self.test_y[idx]\n",
    "\n",
    "        return (features, subject_id), label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(subject_data, device=device, is_train=True)\n",
    "test_dataset = CustomDataset(subject_data, device=device, is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = ([EEG], [subject_id])\n",
    "# label = finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8216])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for i, (feature, label) in enumerate(train_dataloader):\n",
    "    print(feature[0].shape)\n",
    "    print(feature[1].shape)\n",
    "    print(label.shape)\n",
    "    print('---------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.c = input_dim[0]\n",
    "        self.t = input_dim[1]\n",
    "        self.b = input_dim[2]\n",
    "        \n",
    "        self.user_dim = 5\n",
    "        self.num_classes = 5\n",
    "\n",
    "        #user_dim -> condition_dim\n",
    "        # feature_dim -> EEG_dim\n",
    "        self.embed_dim = 16\n",
    "        self.k_dim = 16\n",
    "        self.v_dim = 16\n",
    "\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'reduce': nn.Linear(2,1),\n",
    "            'W_q' : nn.Linear(self.t, self.embed_dim, bias = False),      # Query transformation\n",
    "            'W_k' : nn.Linear(1, self.k_dim, bias = False),   # Key transformation\n",
    "            'W_v': nn.Linear(1, self.v_dim, bias = False),  # Value transformation\n",
    "            'dropout': nn.Dropout(0.2),\n",
    "            'classifier' : nn.Linear(self.c * self.v_dim, self.num_classes)\n",
    "        })\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, features, user_indices):\n",
    "        features = features.reshape(-1,2)\n",
    "        features = self.layers['reduce'](features)         \n",
    "        features = features.view(-1, self.c, self. t)         \n",
    "\n",
    "\n",
    "        # Convert user_indices to one_hot vectors\n",
    "        user_one_hot = torch.zeros(user_indices.size(0), self.user_dim, device=user_indices.device)\n",
    "        user_one_hot.scatter_(1, user_indices.unsqueeze(1), 1)\n",
    "        user_one_hot = torch.unsqueeze(input=user_one_hot, dim=2)\n",
    "      \n",
    "   \n",
    "        # Query Matrix\n",
    "        query = self.layers['W_q'](features)\n",
    "\n",
    "        \n",
    "        # Key Matrix\n",
    "        key = self.layers['W_k'](user_one_hot) \n",
    "        key = torch.transpose(key, 1, 2)\n",
    "\n",
    "\n",
    "        # Attention\n",
    "        attention_scores = torch.bmm(query, key) \n",
    "        attention_scores = attention_scores / math.sqrt(self.embed_dim)\n",
    "        attention_probs = self.softmax(attention_scores) \n",
    "       \n",
    "        # Value Matrix\n",
    "        value = self.layers['W_v'](user_one_hot)\n",
    "      \n",
    "        # Calculate the attended features\n",
    "        attended_features = torch.bmm(attention_probs, value) \n",
    "\n",
    "\n",
    "        # Flatten\n",
    "        batch_size = attended_features.size(0)\n",
    "        attended_features = attended_features.view(batch_size, -1)\n",
    "\n",
    "        # Dropout\n",
    "        attended_features = self.layers['dropout'](attended_features)\n",
    "        \n",
    "        # Classify the attended features\n",
    "        output = self.layers['classifier'](attended_features)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TorchAttention, self).__init__()\n",
    "        self.c = input_dim[0]\n",
    "        self.t = input_dim[1]\n",
    "        self.b = input_dim[2]\n",
    "        \n",
    "        self.user_dim = 5\n",
    "        self.num_classes = 5\n",
    "\n",
    "        #user_dim -> condition_dim\n",
    "        # feature_dim -> EEG_dim\n",
    "        self.embed_dim = 16\n",
    "        self.k_dim = 16\n",
    "        self.v_dim = 16\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=1, batch_first= True, dropout=0.2)\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'reduce': nn.Linear(2,1),\n",
    "            'W_q' : nn.Linear(self.t, self.embed_dim, bias = False),      # Query transformation\n",
    "            'W_k' : nn.Linear(1, self.k_dim, bias = False),   # Key transformation\n",
    "            'W_v': nn.Linear(1, self.v_dim, bias = False),  # Value transformation\n",
    "            'dropout': nn.Dropout(0.2),\n",
    "            'classifier' : nn.Linear(self.c * self.v_dim, self.num_classes)\n",
    "        })\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, features, user_indices):\n",
    "        features = features.reshape(-1,2)\n",
    "        features = self.layers['reduce'](features)         \n",
    "        features = features.view(-1, self.c, self. t)         \n",
    "\n",
    "\n",
    "        # Convert user_indices to one_hot vectors\n",
    "        user_one_hot = torch.zeros(user_indices.size(0), self.user_dim, device=user_indices.device)\n",
    "        user_one_hot.scatter_(1, user_indices.unsqueeze(1), 1)\n",
    "        user_one_hot = torch.unsqueeze(input=user_one_hot, dim=2)\n",
    "      \n",
    "   \n",
    "        # Query Matrix\n",
    "        query = self.layers['W_q'](features)\n",
    "\n",
    "        \n",
    "        # Key Matrix\n",
    "        key = self.layers['W_k'](user_one_hot) \n",
    "        \n",
    "       \n",
    "        # Value Matrix\n",
    "        value = self.layers['W_v'](user_one_hot)\n",
    "      \n",
    "        # Calculate the attended features\n",
    "        attended_features, _ = self.attention(query, key, value)\n",
    "      \n",
    "        # Flatten\n",
    "        batch_size = attended_features.size(0)\n",
    "        attended_features = attended_features.reshape(batch_size, -1)\n",
    "\n",
    "        # Dropout\n",
    "        attended_features = self.layers['dropout'](attended_features)\n",
    "        \n",
    "        # Classify the attended features\n",
    "        output = self.layers['classifier'](attended_features)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "├─MultiheadAttention: 1-1                          --\n",
      "|    └─NonDynamicallyQuantizableLinear: 2-1        272\n",
      "├─ModuleDict: 1-2                                  --\n",
      "|    └─Linear: 2-2                                 3\n",
      "|    └─Linear: 2-3                                 416\n",
      "|    └─Linear: 2-4                                 16\n",
      "|    └─Linear: 2-5                                 16\n",
      "|    └─Dropout: 2-6                                --\n",
      "|    └─Linear: 2-7                                 12,645\n",
      "├─Softmax: 1-3                                     --\n",
      "===========================================================================\n",
      "Total params: 13,368\n",
      "Trainable params: 13,368\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create model and pass data\n",
    "#model = CrossAttention(train_dataset.dim)\n",
    "model = TorchAttention(train_dataset.dim)\n",
    "model.to(device)\n",
    "summary(model, input_size=(5, 10, input_size));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (feature, label) in enumerate(train_dataloader):\n",
    "    model(feature[0], feature[1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dataloader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features,  labels in dataloader:\n",
    "\n",
    "            outputs = model(features = features[0], user_indices = features[1])\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy * 100\n",
    "    #print(f\"Accuracy: {accuracy * 100:.2f}% ({correct_predictions}/{total_predictions})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 91.97771072387695, Train accuracy: 41.70%, Test accuracy: 23.60%\n",
      "Epoch 20/200, Loss: 83.3547922372818, Train accuracy: 48.00%, Test accuracy: 22.00%\n",
      "Epoch 30/200, Loss: 80.10269731283188, Train accuracy: 48.00%, Test accuracy: 19.20%\n",
      "Epoch 40/200, Loss: 78.47107994556427, Train accuracy: 49.80%, Test accuracy: 22.80%\n",
      "Epoch 50/200, Loss: 78.2370075583458, Train accuracy: 48.20%, Test accuracy: 18.80%\n",
      "Epoch 60/200, Loss: 76.95687717199326, Train accuracy: 49.10%, Test accuracy: 20.00%\n",
      "Epoch 70/200, Loss: 75.79242211580276, Train accuracy: 49.80%, Test accuracy: 22.40%\n",
      "Epoch 80/200, Loss: 75.36960899829865, Train accuracy: 52.00%, Test accuracy: 21.20%\n",
      "Epoch 90/200, Loss: 74.15151876211166, Train accuracy: 51.40%, Test accuracy: 19.60%\n",
      "Epoch 100/200, Loss: 74.62020021677017, Train accuracy: 53.60%, Test accuracy: 20.40%\n",
      "Epoch 110/200, Loss: 73.56184983253479, Train accuracy: 52.10%, Test accuracy: 21.20%\n",
      "Epoch 120/200, Loss: 72.79227709770203, Train accuracy: 53.40%, Test accuracy: 18.00%\n",
      "Epoch 130/200, Loss: 71.82732141017914, Train accuracy: 54.00%, Test accuracy: 18.40%\n",
      "Epoch 140/200, Loss: 71.31589269638062, Train accuracy: 53.20%, Test accuracy: 18.40%\n",
      "Epoch 150/200, Loss: 71.15005624294281, Train accuracy: 54.40%, Test accuracy: 17.60%\n",
      "Epoch 160/200, Loss: 70.29299634695053, Train accuracy: 55.30%, Test accuracy: 18.40%\n",
      "Epoch 170/200, Loss: 70.75184684991837, Train accuracy: 54.30%, Test accuracy: 18.80%\n",
      "Epoch 180/200, Loss: 69.55854099988937, Train accuracy: 55.90%, Test accuracy: 17.60%\n",
      "Epoch 190/200, Loss: 69.61242294311523, Train accuracy: 55.70%, Test accuracy: 20.80%\n",
      "Epoch 200/200, Loss: 69.20055419206619, Train accuracy: 55.80%, Test accuracy: 17.60%\n",
      "##################################################\n",
      "Final_loss: 69.20055419206619\n",
      "Final train accuracy: 55.80%\n",
      "Final test accuracy: 17.60%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_features, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features = batch_features[0], user_indices = batch_features[1])\n",
    "\n",
    "        loss = criterion(outputs, batch_labels.long())\n",
    "          \n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "   \n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        train_accuracy = accuracy(train_dataloader)\n",
    "        test_accuracy = accuracy(test_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}, Train accuracy: {train_accuracy:.2f}%, Test accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(f'Final_loss: {epoch_loss}')\n",
    "print(f'Final train accuracy: {accuracy(train_dataloader):.2f}%')\n",
    "print(f'Final test accuracy: {accuracy(test_dataloader):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cefe38f745df9e33a66570f2e5a410ba71c4ae3bf929b6ad1b474ac5f904d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
