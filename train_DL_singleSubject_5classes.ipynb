{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "import mne\n",
    "\n",
    "mne.set_log_level('error')\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "from utils.load import Load\n",
    "from utils.eval import accuracy\n",
    "from config.default import cfg\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index (50, 158, 30, 2)\n",
      "little (50, 158, 30, 2)\n",
      "middle (50, 158, 30, 2)\n",
      "ring (50, 158, 30, 2)\n",
      "thumb (50, 158, 30, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the data  from the HDF5 file\n",
    "target_dir = 'features'\n",
    "tag = '0_25powers' # with_bad\n",
    "file_path = os.path.join(target_dir, tag+'_'+cfg['subjects'][subject_id] + '.h5')\n",
    "\n",
    "\n",
    "data = {}\n",
    "with h5py.File(file_path, 'r') as h5file:\n",
    "    for key in h5file.keys():\n",
    "        data[key] = np.array(h5file[key])\n",
    "\n",
    "# Print the loaded data dictionary\n",
    "for key, value in data.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([200, 9480])\n",
      "Train labels shape: torch.Size([200])\n",
      "Test data shape: torch.Size([50, 9480])\n",
      "Test labels shape: torch.Size([50])\n",
      "Train data shape: torch.Size([200, 9480])\n",
      "Train labels shape: torch.Size([200])\n",
      "Test data shape: torch.Size([50, 9480])\n",
      "Test labels shape: torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, flatten = False, normalize = False, train_percent=0.8, seed=42, device=None, is_train=True):\n",
    "        self.device = device\n",
    "        self.is_train = is_train\n",
    "        self.flatten = flatten\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.train_X, self.train_y, self.test_X, self.test_y = self.preprocess_data(data, train_percent, seed)\n",
    "        self.train_X = torch.tensor(self.train_X, dtype=torch.float32, device=self.device)\n",
    "        self.train_y = torch.tensor(self.train_y, dtype=torch.long, device=self.device)\n",
    "        self.test_X = torch.tensor(self.test_X, dtype=torch.float32, device=self.device)\n",
    "        self.test_y = torch.tensor(self.test_y, dtype=torch.long, device=self.device)\n",
    "\n",
    "        print(f'Train data shape: {self.train_X.shape}')\n",
    "        print(f'Train labels shape: {self.train_y.shape}')\n",
    "        print(f'Test data shape: {self.test_X.shape}')\n",
    "        print(f'Test labels shape: {self.test_y.shape}')\n",
    "        self.dim = self.train_X[0].shape.numel()\n",
    "\n",
    "    def get_dim():\n",
    "        return self.dim\n",
    "\n",
    "    def preprocess_data(self, data, train_percent, seed):\n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "        test_features = []\n",
    "        test_labels = []\n",
    "\n",
    "        for i, finger in enumerate(data):\n",
    "            features = data[finger]\n",
    "            if self.flatten:\n",
    "                features = features.reshape(features.shape[0], -1)\n",
    "            if self.normalize:\n",
    "                original_shape = features.shape\n",
    "                features = features.reshape(features.shape[0], -1)\n",
    "                features = StandardScaler().fit_transform(features)\n",
    "                features = features.reshape(original_shape)\n",
    "\n",
    "\n",
    "            # Generate labels\n",
    "            labels = torch.tensor(np.ones((len(features))) * i)\n",
    "\n",
    "            \n",
    "            \n",
    "            train_features.extend(features[:int(len(features) * train_percent)])\n",
    "            train_labels.extend(labels[:int(len(features) * train_percent)])\n",
    "            test_features.extend(features[int(len(features) * train_percent):])\n",
    "            test_labels.extend(labels[int(len(labels) * train_percent):])\n",
    "\n",
    "         \n",
    "        train_features = np.stack(arrays=train_features, axis=0)\n",
    "        train_labels = np.stack(arrays=train_labels, axis=0)\n",
    "        test_features = np.stack(arrays=test_features, axis=0)\n",
    "        test_labels = np.stack(arrays=test_labels, axis=0)\n",
    "\n",
    "        return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_y) if self.is_train else len(self.test_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.get_train_item(idx)\n",
    "        else:\n",
    "            return self.get_test_item(idx)\n",
    "\n",
    "    def get_train_item(self, idx):\n",
    "        features = self.train_X[idx]\n",
    "        label = self.train_y[idx]\n",
    "\n",
    "        return features, label\n",
    "\n",
    "    def get_test_item(self, idx):\n",
    "        features = self.test_X[idx]\n",
    "        label = self.test_y[idx]\n",
    "\n",
    "        return features, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(data, flatten = True, normalize = False, device=device, is_train=True)\n",
    "test_dataset = CustomDataset(data, flatten = True, normalize = False, device=device, is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 9480])\n",
      "torch.Size([50])\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Test data loader\n",
    "for i, (feature, label) in enumerate(train_dataloader):\n",
    "    print(feature.shape)\n",
    "    print(label.shape)\n",
    "    print('---------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation):\n",
    "        super(SingleLayerMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            47,405\n",
      "├─Linear: 1-2                            30\n",
      "├─BatchNorm1d: 1-3                       10\n",
      "├─Dropout: 1-4                           --\n",
      "├─ReLU: 1-5                              --\n",
      "=================================================================\n",
      "Total params: 47,445\n",
      "Trainable params: 47,445\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = SingleLayerMLP(train_dataset.dim, 5, 5,  nn.ReLU())\n",
    "\n",
    "model.to(device)\n",
    "summary(model, input_size=(5, 10, *next(iter(train_dataloader))[0][0].shape));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 3.3863431811332703, Train accuracy: 64.50%, Test accuracy: 36.00%\n",
      "Epoch 20/200, Loss: 1.2877494990825653, Train accuracy: 97.50%, Test accuracy: 24.00%\n",
      "Epoch 30/200, Loss: 0.5222935155034065, Train accuracy: 99.50%, Test accuracy: 24.00%\n",
      "Epoch 40/200, Loss: 0.5021945312619209, Train accuracy: 99.50%, Test accuracy: 20.00%\n",
      "Epoch 50/200, Loss: 0.4554506689310074, Train accuracy: 99.50%, Test accuracy: 20.00%\n",
      "Epoch 60/200, Loss: 0.44104553014039993, Train accuracy: 99.50%, Test accuracy: 22.00%\n",
      "Epoch 70/200, Loss: 0.41934122145175934, Train accuracy: 99.50%, Test accuracy: 28.00%\n",
      "Epoch 80/200, Loss: 0.413548544049263, Train accuracy: 99.50%, Test accuracy: 24.00%\n",
      "Epoch 90/200, Loss: 0.40208467841148376, Train accuracy: 99.50%, Test accuracy: 26.00%\n",
      "Epoch 100/200, Loss: 0.39754699915647507, Train accuracy: 99.50%, Test accuracy: 28.00%\n",
      "Epoch 110/200, Loss: 0.38988155871629715, Train accuracy: 99.50%, Test accuracy: 26.00%\n",
      "Epoch 120/200, Loss: 0.38674963265657425, Train accuracy: 99.50%, Test accuracy: 30.00%\n",
      "Epoch 130/200, Loss: 0.38094688951969147, Train accuracy: 99.00%, Test accuracy: 30.00%\n",
      "Epoch 140/200, Loss: 0.3646075278520584, Train accuracy: 99.50%, Test accuracy: 28.00%\n",
      "Epoch 150/200, Loss: 0.3709905371069908, Train accuracy: 99.50%, Test accuracy: 30.00%\n",
      "Epoch 160/200, Loss: 0.3949693366885185, Train accuracy: 99.50%, Test accuracy: 28.00%\n",
      "Epoch 170/200, Loss: 0.3630891889333725, Train accuracy: 99.50%, Test accuracy: 30.00%\n",
      "Epoch 180/200, Loss: 0.3597528859972954, Train accuracy: 99.50%, Test accuracy: 28.00%\n",
      "Epoch 190/200, Loss: 0.3802850767970085, Train accuracy: 99.50%, Test accuracy: 32.00%\n",
      "Epoch 200/200, Loss: 0.3945276215672493, Train accuracy: 99.50%, Test accuracy: 24.00%\n",
      "##################################################\n",
      "Final_loss: 0.3945276215672493\n",
      "Final train accuracy: 99.50%\n",
      "Final test accuracy: 24.00%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 200\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_features, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        loss = criterion(outputs, batch_labels.long())\n",
    "          \n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "   \n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        train_accuracy = accuracy(model, train_dataloader)\n",
    "        test_accuracy = accuracy(model, test_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss}, Train accuracy: {train_accuracy:.2f}%, Test accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "print(\"#\"*50)\n",
    "print(f'Final_loss: {epoch_loss}')\n",
    "print(f'Final train accuracy: {accuracy(model, train_dataloader):.2f}%')\n",
    "print(f'Final test accuracy: {accuracy(model, test_dataloader):.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam optimalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, test_dataloader, model, criterion, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_features, batch_labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return accuracy(model, test_dataloader)\n",
    "\n",
    "def objective(trial, train_dataloader, test_dataloader):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 100, 2000)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128)\n",
    "    activation_name = trial.suggest_categorical(\"activation\", [\"relu\", \"elu\", \"leaky_relu\"])\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    if activation_name == \"relu\":\n",
    "        activation = nn.ReLU()\n",
    "    elif activation_name == \"elu\":\n",
    "        activation = nn.ELU()\n",
    "    elif activation_name == \"leaky_relu\":\n",
    "        activation = nn.LeakyReLU()\n",
    "\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = optim.SGD\n",
    "    elif optimizer == \"Adam\":\n",
    "        optimizer = optim.Adam\n",
    "\n",
    "    \n",
    "    model = SingleLayerMLP(train_dataset.dim, hidden_size, 5, activation)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    return train(train_dataloader, test_dataloader, model, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "\n",
    "def train_MLP(n_trials = 100):\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial,train_dataloader, test_dataloader), n_trials=n_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    print(f'Best trial params: {best_trial.params}')\n",
    "    print(f'Best trial accuracy: {best_trial.value :.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-27 20:55:03,981]\u001b[0m A new study created in memory with name: no-name-a0aa4f6c-3abf-43ed-92ba-50d6cb10f0b5\u001b[0m\n",
      "\u001b[32m[I 2023-04-27 20:55:05,308]\u001b[0m Trial 0 finished with value: 32.0 and parameters: {'learning_rate': 0.09902304412740562, 'num_epochs': 195, 'hidden_size': 98, 'activation': 'elu', 'optimizer': 'SGD', 'weight_decay': 1.9466944305815796e-05}. Best is trial 0 with value: 32.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial params: {'learning_rate': 0.09902304412740562, 'num_epochs': 195, 'hidden_size': 98, 'activation': 'elu', 'optimizer': 'SGD', 'weight_decay': 1.9466944305815796e-05}\n",
      "Best trial accuracy: 32.00%\n"
     ]
    }
   ],
   "source": [
    "train_MLP(n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntag = 'gpt4freq_all'\\nBest trial params: {'learning_rate': 0.009669058999906542, 'num_epochs': 1671, 'hidden_size': 111, 'activation': 'relu', 'optimizer': 'SGD'}\\nBest trial accuracy: 48.00%\\n\\ntag = 'reproduced_with_bad'\\nBest trial params: {'learning_rate': 0.0021345711699235683, 'num_epochs': 732, 'hidden_size': 37, 'activation': 'relu', 'optimizer': 'Adam'}\\nBest trial accuracy: 48.00%\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tag = 'gpt4freq_all'\n",
    "Best trial params: {'learning_rate': 0.009669058999906542, 'num_epochs': 1671, 'hidden_size': 111, 'activation': 'relu', 'optimizer': 'SGD'}\n",
    "Best trial accuracy: 48.00%\n",
    "\n",
    "tag = 'reproduced_with_bad'\n",
    "Best trial params: {'learning_rate': 0.0021345711699235683, 'num_epochs': 732, 'hidden_size': 37, 'activation': 'relu', 'optimizer': 'Adam'}\n",
    "Best trial accuracy: 48.00%\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cefe38f745df9e33a66570f2e5a410ba71c4ae3bf929b6ad1b474ac5f904d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
